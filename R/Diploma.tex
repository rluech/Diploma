\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={Predicting Household Composition by TV Viewing Behavior},
            pdfauthor={Rafael Lüchinger},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\providecommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{Predicting Household Composition by TV Viewing Behavior}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
  \subtitle{Diploma of Advanced Studies in Applied Statistics at ETH Zurich}
  \author{Rafael Lüchinger}
    \preauthor{\centering\large\emph}
  \postauthor{\par}
      \predate{\centering\large\emph}
  \postdate{\par}
    \date{24 April 2019}

\usepackage{float}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{2}
\tableofcontents
}
\hypertarget{summary}{%
\section{Summary}\label{summary}}

In this study TV viewing data from individuals was aggregated to
household level to find out if the household size can be predicted based
on the households TV viewing. This simulates the obstacle that has to be
mastered when RPD data has to be integrated with traditional panel data.
For each home the average daily viewing duration in seconds across an 8
week period was extracted for 53 different features such as daytime,
weekday, type of channel and program genre. The sample of 2006 homes
were split by a ratio of 6:4 and used for training and cross validating,
respectively. Three different machine learning algorithms were applied,
multinomial linear regression, support vector machine and random forest.
All three classifier performed similarly poor on test data with an
overall accuracy of 42\% to 44\%, given a baseline accuracy of 27\%.
Separating small (household size 1 and 2) versus bigger homes (household
size 3, 4 and 5) was the main pattern found and features like viewing on
kids channel and kids programs among others was found to be particularly
important. We conclude that a households TV consumption is not
informative enough to derive the number of individuals living in that
home.

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

TV audience in Switzerland is measured by
\href{https:://www.mediapulse.ch/en}{Mediapulse AG}. A representative
\href{https:://www.mediapulse.ch/en/tv/research-method/the-panel.html}{panel}
of roughly 2000 households is constantly under
\href{https:://www.mediapulse.ch/en/tv/research-method/the-measuring-technique.html}{measurement}.
These homes were carefully selected by a complex sampling design and all
household members have agreed to be part of the study. The TV viewing of
each household member is individually recorded and detailed demographics
are known for each person. This allows the market to target TV audiences
by relevant characteristics like age gender and many more.

One issue with the panel approach is poor granularity. That means
sometimes the system can not provide any audience figures for a specific
channel or airtime. It is likely that in the Swiss population of about
3.5 million households at least a few people are watching even exotic
programs at exotic times of the day. However, out of a panel of 2000
households chances are high that no one was watching that content. This
is not a bias of the measurement but poor resolution.

A solution to this problem could be the inclusion of third party data.
Set-Top-Boxes (STB) of TV-provider (Swisscom, UPC, etc.) are
automatically recording the TV consumption in millions of Swiss homes
and the data is returned to the providers servers (return path data,
RPD). There are still many issues with these data that are currently
addressed.

One major issue of RPD is that the viewing data is on household level,
not on individual level. Household-level data is of little use to the
market. Because it gives no insight in target groups based on age and
gender and alike.

It is unlikely that RPD provider will ever measure the individual
viewing or survey individual demographics within the subscribers homes.
Apart from region code, the only information about the home is the
viewing data itself. So the question arises if it is possible to predict
the household composition based on viewing behavior.

The aim of this study is to explore the possibility to predict the
household composition within a household using TV viewing data. It seems
to be a two-step-problem, first to find the number of household members
and then to assign age and gender to the individuals.

We will use the \emph{Mediapulse TV-Panel} and its viewing data to study
the subject. For all households in the panel its composition including
household size and age and sex of each person is known. For each panel
home the viewing data will be aggregated to household level. Different
supervised machine learning algorithms will be fed with features
extracted from that household viewing data.

\hypertarget{tv-viewing-data}{%
\section{TV Viewing Data}\label{tv-viewing-data}}

Mediapulse TV viewing raw-data comes in the form of daily text files.
There are three types of files:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{dem}: all individuals with their demographics and daily
  weights
\item
  \texttt{view}: the TV viewing (live and time-shifted viewing)
\item
  \texttt{prog}: the program timetable with genre information
\end{enumerate}

A commercial software allows to analyse this data via an easy to use
software tool. The output of this Software is the official Swiss TV
audience measure published by Mediapulse AG and accepted by the market.
I have written an R-package that allows to read and analyse the very
same input raw-data and output the very same results (e.g.~daily
estimates, the so called ``Facts'' like \emph{Reach}, \emph{Rating} or
\emph{Share}, etc.). The fact that the results between Software and
R-package match precisely, guarantees that the data processing in R is
correct, plus it allows to check if all calculations and aggregations
are correctly implemented as intended.

\begin{longtable}[]{@{}lrrlll@{}}
\caption{\label{tab:tab1}TV viewing raw-data (simplified). Reading
example: On day \texttt{2018-01-01}, in household \texttt{2381}
individual \texttt{1} is watching channel \texttt{SRF\ 1} from
\texttt{18:04:21} to \texttt{18:13:02}. Later that day this person
switches to channel \texttt{ARTE} and is joined by another household
member individual \texttt{2}.}\tabularnewline
\toprule
day & hh & ind & chn & start & end\tabularnewline
\midrule
\endfirsthead
\toprule
day & hh & ind & chn & start & end\tabularnewline
\midrule
\endhead
2018-01-01 & 2381 & 1 & SRF 1 & 18:04:21 & 18:13:02\tabularnewline
2018-01-01 & 2381 & 1 & ARTE & 18:45:20 & 20:05:45\tabularnewline
2018-01-01 & 2381 & 2 & ARTE & 18:45:20 & 19:45:03\tabularnewline
\bottomrule
\end{longtable}

Demographic information is simply joined on keys \texttt{day},
\texttt{hh} and \texttt{ind}. Program schedule is joined via an overlap
join on keys \texttt{day}, \texttt{channel} and
\texttt{start}/\texttt{end}. If a viewing statement overlaps with
multiple programs, the statement gets duplicated and the
\texttt{start}/\texttt{end} intervals needs to be cropped to the viewing
interval boundaries.

\hypertarget{target-and-feature}{%
\section{Target and Feature}\label{target-and-feature}}

\hypertarget{target-household-composition}{%
\subsection{Target: Household
Composition}\label{target-household-composition}}

The aim of this study is to predict the household composition in form of
the household size, e.g.~the number of people living in the household.
Our sample for this study comprises 2006 homes in which a total of 4388
individuals are living in. This gives an average household size of 2.19.

The variable \emph{hhsize} is given in the demographics file of the TV
raw-data. \emph{hhsize} is not necessarily equal to the sum of
individuals for the following reasons:

\begin{itemize}
\tightlist
\item
  children 0-2 years old are not recorded (this is a market convention)
\item
  guest are part of the TV-panel but not counted for household size
\item
  household size is coded 1, 2, 3, 4, 5+, with 5+ meaning households
  with 5 or more members
\end{itemize}

Another detail is that household size is not necessarily constant over
time. The number of people living in a household may change by natural
reasons like birth, death, moving in or out. We use the hhsize of a
single sample day and assume the household size is constant for the
period of viewing data we are using.

\begin{longtable}[]{@{}lrrrrrrrrrrrr@{}}
\caption{Household composition is the sociodemografic profile of a
household, for example, household size, and age and gender of the
household members. The last 3 of our sample of 2006 households are
shown. For this study the target to predict is
\emph{hhsize}.}\tabularnewline
\toprule
& hh & hhsize & age\_1 & age\_2 & age\_3 & age\_4 & age\_5 & sex\_1 &
sex\_2 & sex\_3 & sex\_4 & sex\_5\tabularnewline
\midrule
\endfirsthead
\toprule
& hh & hhsize & age\_1 & age\_2 & age\_3 & age\_4 & age\_5 & sex\_1 &
sex\_2 & sex\_3 & sex\_4 & sex\_5\tabularnewline
\midrule
\endhead
2004 & 6200 & 2 & 63 & 72 & 0 & 0 & 0 & 2 & 1 & 0 & 0 & 0\tabularnewline
2005 & 6201 & 2 & 71 & 73 & 0 & 0 & 0 & 1 & 2 & 0 & 0 & 0\tabularnewline
2006 & 6204 & 4 & 52 & 47 & 17 & 13 & 0 & 1 & 2 & 1 & 2 &
0\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{generating-features-of-viewing-behavior}{%
\subsection{Generating Features of Viewing
Behavior}\label{generating-features-of-viewing-behavior}}

\hypertarget{selecting-8-weeks-of-viewing-data}{%
\subsubsection{Selecting 8 Weeks of Viewing
Data}\label{selecting-8-weeks-of-viewing-data}}

For this study, a sample day was fixed, and the viewing data of all
panel member present at that day is collected 4 weeks prior and 4 weeks
after that date. The sample day is the Sunday \texttt{2017-11-12} and
comprises 2006 households and 4388 individuals respectively.

The period of eight weeks should be long enough to reflect individual
viewing behavior. Autumn was chosen because during colder months people
are watching more TV than in Summer. This period is free of holidays or
unusual TV events (FIFA World cup, etc.). Within the 7 * 8 = 56 days,
the weekdays Mondays to Sundays are balanced. This is relevant as TV
viewing differs significantly between weekends and workdays (Figure
\ref{fig:fig1}).

\hypertarget{tv-viewing-on-household-level}{%
\subsubsection{TV Viewing on Household
Level}\label{tv-viewing-on-household-level}}

The TV raw-data described earlier shows that \emph{Mediapulse TV data}
is recorded for each individual. With RPD data however this is not the
case. RPD data only provide viewing data on household level. Which
person, or how many person are sitting in front of the TV set is
unknown. Also, there is no demographic information accompanying RPD
data. Here we study if it is possible to predict at least the number of
household members if only TV viewing on household level is known, like
with RPD data. To this end the \emph{Mediapulse TV data} have to be
aggregated form individual level to household level. This means, if more
than one person is watching the same content, on household level, this
is reflected by a single viewing statement. This household aggregation
algorithm is somewhat more complex, but not relevant here.

\hypertarget{feature-generation}{%
\subsubsection{Feature Generation}\label{feature-generation}}

Features are a set of variables that are used as input data for Machine
Learning Classifiers or as predictors for statistical models. In both
cases we aim to predict the target variable \emph{hhsize}. To create
features of viewing behavior the TV data on household level is summed up
for each household and day, by different characteristics. Then, for each
household the average across the 56 days was calculated. TV viewing is
expressed as the duration of viewing in seconds.

The characteristics underlying the feature generation is guided by
industry knowledge and intuition about TV viewing behavior we believe
would carry information about the household composition, i.e:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Dimension time

  \begin{itemize}
  \tightlist
  \item
    weekend vs.~working days
  \item
    time of the day
  \end{itemize}
\item
  Dimension content

  \begin{itemize}
  \tightlist
  \item
    type of channel
  \item
    type of program genre
  \end{itemize}
\end{enumerate}

Figure \ref{fig:fig1}, and Figure \ref{fig:fig2} in the Appendix are
illustrating the effect of the dimension \emph{time} on total TV
viewing.

There are over 300 TV channels received in Switzerland. In comparison to
other countries this so called \emph{inspill} is very large. Because of
the small size of Switzerland and its different linguistic regions there
are many foreign channels being watched from the neighboring countries.
For simplification the channels have been mapped to channel groups.
There are 3 type of groups: channel type, language, and country of
origin.

The program genre is a pre-specified variable in the program schedule
files. Each program is categorized to one of the 14 different genres.
The TV viewing data is overlapped and split up by the program schedule.
The viewing duration is than summed up by each program genre within each
household.

\begin{longtable}[]{@{}lll@{}}
\caption{The sets of features of TV viewing behavior to predict
household composition.}\tabularnewline
\toprule
weekpart by time of day & channels & programs\tabularnewline
\midrule
\endfirsthead
\toprule
weekpart by time of day & channels & programs\tabularnewline
\midrule
\endhead
day\_weekend\_02to06 & chn\_arts & prg\_commercial\tabularnewline
day\_weekend\_06to08 & chn\_generalistprivate & prg\_info\tabularnewline
day\_weekend\_08to11 & chn\_generalistpublic & prg\_kids\tabularnewline
day\_weekend\_11to13 & chn\_kids & prg\_missing\tabularnewline
day\_weekend\_13to17 & chn\_livestileindoor & prg\_movie\tabularnewline
day\_weekend\_17to20 & chn\_livestileoutdoor & prg\_music\tabularnewline
day\_weekend\_20to22 & chn\_local & prg\_news\tabularnewline
day\_weekend\_22to24 & chn\_movieseries & prg\_other\tabularnewline
day\_weekend\_24to02 & chn\_music & prg\_series\tabularnewline
day\_workday\_02to06 & chn\_nature & prg\_service\tabularnewline
day\_workday\_06to08 & chn\_news & prg\_show\tabularnewline
day\_workday\_08to11 & chn\_paytv & prg\_sport\tabularnewline
day\_workday\_11to13 & chn\_religion & prg\_talk\tabularnewline
day\_workday\_13to17 & chn\_sport & prg\_trailer\tabularnewline
day\_workday\_17to20 & chn\_foreign &\tabularnewline
day\_workday\_20to22 & chn\_swiss &\tabularnewline
day\_workday\_22to24 & chn\_english &\tabularnewline
day\_workday\_24to02 & chn\_french &\tabularnewline
& chn\_german &\tabularnewline
& chn\_italian &\tabularnewline
& chn\_other &\tabularnewline
\bottomrule
\end{longtable}

\begin{longtable}[]{@{}rrrrr@{}}
\caption{Final input data set. Shown are the first 6 rows and the first
5 columns. Each of the 2006 households is an observation on rows and
identified by the household ID \emph{hh}. The household size, the target
to predict, is given in column \emph{hhsize}. All 53 above mentioned
features are given in the following columns. The values are the average
daily viewing duration in seconds by feature on household
level.}\tabularnewline
\toprule
hh & hhsize & day\_weekend\_02to06 & day\_weekend\_06to08 &
day\_weekend\_08to11\tabularnewline
\midrule
\endfirsthead
\toprule
hh & hhsize & day\_weekend\_02to06 & day\_weekend\_06to08 &
day\_weekend\_08to11\tabularnewline
\midrule
\endhead
6 & 2 & 0.0000 & 0.0000 & 372.8750\tabularnewline
9 & 4 & 88.3125 & 20.7500 & 621.5000\tabularnewline
14 & 2 & 328.1250 & 39.2500 & 12.0000\tabularnewline
20 & 2 & 1019.6667 & 555.1333 & 824.4667\tabularnewline
21 & 2 & 607.3750 & 917.2500 & 3143.5000\tabularnewline
35 & 1 & 72.0000 & 357.7500 & 2303.3125\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{data-exploration}{%
\section{Data Exploration}\label{data-exploration}}

\hypertarget{classification-versus-regression}{%
\subsection{Classification versus
Regression}\label{classification-versus-regression}}

Theoretically household size can be interpret both as five separate
categories or as a scale (ordinal or proportional) ranging from 1 to 5.
For this study we interpret household size as categorical variable and
therefor apply classification not regression. One reason is that behind
a specific household size very different types of household compositions
may exist. For example a two person household could consist of an
elderly couple, a two young students of the same gender, a single parent
with a child, etc. Accordingly the TV viewing behavior in households of
the same size may differ significantly given the possible variety of
demographics profiles.

\hypertarget{log-transformation}{%
\subsection{Log Transformation}\label{log-transformation}}

Screening through the values of the 53 feature variables reveals that
often the viewing duration is strongly right skewed and zero inflated
(see Figure \ref{fig:fig3} in the Appendix). That means most households
have a relatively low value of TV viewing duration but for a few
households the viewing is relatively high. A log transformation makes
the data more symmetric. Although in general for Machine Learning
algorithms such a transformation is unnecessary, it should neither be
harmful, and we continue with log transformed data. Because of many zero
values we use the transformation `log(x + 1)'.

\hypertarget{visualization-of-features}{%
\subsection{Visualization of Features}\label{visualization-of-features}}

Before applying statistical methods, we can simply visually explore if
there is a predictor variable that separates well the households by
household size. Figure \ref{fig:fig4} in the Appendix shows some
examples. For most features there is no such discrimination power
apparent. But the amount of viewing kinds channels seems to separate
small and bigger households.

Another option is to visualize the correlation between feature and
between features and the target variable. Figure \ref{fig:fig5} in the
Appendix shows a heat map of the correlation matrix of the input data.
The correlation is a standardized measure for the similarity between
features between. The correlation with the target variable reflects how
strong a linear relation ship exists between a particular feature and
household sizes 1 to 5. This is only of limited use as we're also
interested in any non-linear relation with the five household sizes
which we interprets as nominal classes here.

The dendrogram attached to the heat map in Figure \ref{fig:fig5} uses
the correlation structure to visualize the between-features association
in a hierarchical order.

\hypertarget{dimensionality-reduction}{%
\subsection{Dimensionality Reduction}\label{dimensionality-reduction}}

The 53 features are not a lot and far less than the 2006 observations.
Still, some of these variables may not carry much information or they
are redundant to other variables (e.g.~highly correlated). For an
example of dimensionality reduction Principal Component Analysis (PCA)
was chosen. PCA was calculated using the `prcomp' package with centering
but no scaling of the log transformed data matrix. All the features
represent the same unit of viewing duration. Figure \ref{fig:fig6} and
\ref{fig:fig7} in the Appendix shows the results of the PCA.

If PCA finds a few principal components that together explain a big
portion of the total variance, these synthesized features could replace
the original input data. The whole effects on the target variable could
be summarized by 2 or 3 (orthogonal) dimensions.

However, Figure \ref{fig:fig6} shows no such simple decomposition. There
is a dominant first component explaining 30\% of the variance. Most
likely this component reflects the magnitude of overall TV consumption.
All following components only explain a fraction. To reach a cumulative
proportion of about 80\% more than ten components are needed. Given the
general difficulty of a real-world interpretation of PCs, working with
the PCA transformed data is of little use here.

\begin{longtable}[]{@{}lrrr@{}}
\caption{\label{tab:tab5} First 6 PCs. The Variance in the feature
matrix is not easily separated in orthogonal components. To reach 80\%
cumulative explained variance the first 16 PCs would be
needed.}\tabularnewline
\toprule
& Standard deviation & Proportion of Variance & Cumulative
Proportion\tabularnewline
\midrule
\endfirsthead
\toprule
& Standard deviation & Proportion of Variance & Cumulative
Proportion\tabularnewline
\midrule
\endhead
PC1 & 3.982955 & 0.29932 & 0.29932\tabularnewline
PC2 & 2.272029 & 0.09740 & 0.39672\tabularnewline
PC3 & 1.759215 & 0.05839 & 0.45511\tabularnewline
PC4 & 1.517791 & 0.04347 & 0.49858\tabularnewline
PC5 & 1.480727 & 0.04137 & 0.53995\tabularnewline
PC6 & 1.324283 & 0.03309 & 0.57303\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{probability-to-correctly-classify-by-chance}{%
\subsection{Probability to Correctly Classify by
Chance}\label{probability-to-correctly-classify-by-chance}}

The probability to correctly classify by chance is 20\%, if all five
levels of household size were uniformly distributed (naive estimation).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{replicate}\NormalTok{(}\DecValTok{100}\NormalTok{, }\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{, }\KeywordTok{nrow}\NormalTok{(hh.composition), }\DataTypeTok{replace =} \OtherTok{TRUE}\NormalTok{))}
\KeywordTok{round}\NormalTok{(}\KeywordTok{mean}\NormalTok{(}\KeywordTok{apply}\NormalTok{(x, }\DecValTok{2}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(y) }\KeywordTok{mean}\NormalTok{(y }\OperatorTok{==}\StringTok{ }\NormalTok{hh.composition}\OperatorTok{$}\NormalTok{hhsize))), }\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.2
\end{verbatim}

However, we know the probability of each household size level in our
sample:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p <-}\StringTok{ }\KeywordTok{prop.table}\NormalTok{(}\KeywordTok{table}\NormalTok{(hh.composition}\OperatorTok{$}\NormalTok{hhsize))}
\KeywordTok{c}\NormalTok{(}\DataTypeTok{hhsize =} \KeywordTok{round}\NormalTok{(p}\OperatorTok{*}\DecValTok{100}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## hhsize.1 hhsize.2 hhsize.3 hhsize.4 hhsize.5 
##    34.55    32.70    13.71    13.36     5.68
\end{verbatim}

Therefore we can calculate the probability of correct classification by
chance more precisely:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{round}\NormalTok{(}\KeywordTok{sum}\NormalTok{(p}\OperatorTok{^}\DecValTok{2}\NormalTok{), }\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.27
\end{verbatim}

The estimation by simulation comes close:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{replicate}\NormalTok{(}\DecValTok{100}\NormalTok{, }\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{, }\KeywordTok{nrow}\NormalTok{(hh.composition), }\DataTypeTok{replace =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{prob =}\NormalTok{ p))}
\KeywordTok{round}\NormalTok{(}\KeywordTok{mean}\NormalTok{(}\KeywordTok{apply}\NormalTok{(x, }\DecValTok{2}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(y) }\KeywordTok{mean}\NormalTok{(y }\OperatorTok{==}\StringTok{ }\NormalTok{hh.composition}\OperatorTok{$}\NormalTok{hhsize))), }\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.27
\end{verbatim}

\hypertarget{partitioning-into-train-and-test-data}{%
\subsection{Partitioning into Train and Test
Data}\label{partitioning-into-train-and-test-data}}

The 2006 households are split into two datasets, one for training and
one for testing. The training data makes 60\% of households and will be
used to train the different models. The test data consists of the other
40\% of households and will be used for cross validation, .i.e. to test
how good the trained models perform when classifying new data.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(caret)}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{999}\NormalTok{)}
\NormalTok{train <-}\StringTok{ }\NormalTok{caret}\OperatorTok{::}\KeywordTok{createDataPartition}\NormalTok{(pred.log}\OperatorTok{$}\NormalTok{hhsize, }\DataTypeTok{p =} \FloatTok{.6}\NormalTok{, }\DataTypeTok{list =} \OtherTok{FALSE}\NormalTok{)}
\NormalTok{d <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DataTypeTok{train =}\NormalTok{ pred.log[train,}\OperatorTok{-}\DecValTok{1}\NormalTok{], }\DataTypeTok{test =}\NormalTok{ pred.log[}\OperatorTok{-}\NormalTok{train,}\OperatorTok{-}\DecValTok{1}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

A stratified random sampling is used to split the households.
Stratification by household size guarantees that the distribution of
household size is the same between train and test data. So the cross
validation is more reliable.

\begin{longtable}[]{@{}lrrrr@{}}
\caption{\label{tab:tab6} The 2006 household are split randomly into
train (60\%) and test (40\%) data, with stratification by household
size.}\tabularnewline
\toprule
& train & test & train \% & test \%\tabularnewline
\midrule
\endfirsthead
\toprule
& train & test & train \% & test \%\tabularnewline
\midrule
\endhead
hhsize1 & 416 & 277 & 0.35 & 0.35\tabularnewline
hhsize2 & 394 & 262 & 0.33 & 0.33\tabularnewline
hhsize3 & 165 & 110 & 0.14 & 0.14\tabularnewline
hhsize4 & 161 & 107 & 0.13 & 0.13\tabularnewline
hhsize5 & 69 & 45 & 0.06 & 0.06\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{model-specification}{%
\section{Model Specification}\label{model-specification}}

\hypertarget{multinomial-logistic-regression}{%
\subsection{Multinomial Logistic
Regression}\label{multinomial-logistic-regression}}

\hypertarget{full-model}{%
\subsubsection{Full Model}\label{full-model}}

The multinomial logistic regression models the log odds of the five
household size levels as a linear combination of the predictor
variables. We will use the full model with all features when assessing
the prediction performance.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(nnet)}
\NormalTok{m.mnr <-}\StringTok{ }\KeywordTok{multinom}\NormalTok{(hhsize }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ d}\OperatorTok{$}\NormalTok{train, }\DataTypeTok{trace =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{maxit =} \DecValTok{500}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{aic-optimized-model}{%
\subsubsection{AIC Optimized Model}\label{aic-optimized-model}}

In contrast to Machine Learning algorithms (ML), a statistical model
describes the relationship between predictors and outcome variable as a
mathematical formula. That equation is fitted to the data in the best
possible way. However if the formula does not describe the true
relationship the result remains sub optimal. There are many possible
ways of specifying a model, including transformation of predictors
(non-linear relationship) and specification of interactions between
predictors (depending on the state of other predictors). To find the
best model in statistical terms, all models have to be fitted and
compared against each other. A good model in statistical terms is a
model that explains as much variance of the target variable as possible
with the smallest number of predictors. The AIC criterion assesses each
model by the explained variance subtracted by a punishing factor for
each additional model term.

Each model is basically a hypothesis fitted and tested against the data.
Running through all possible models to approach the best one is
cumbersome and time consuming If the true relationship between
predictors and outcome variable is not among our pre-defined set of
models we will not find the optimum and prediction will be sub optimal.
But the advantage over ML is that once a satisfying model is found, the
relation between predictors and outcome variable follows a mathematical
equation. The modelling and prediction is parametric and fully
transparent.

We will use an automated stepwise model search to find a good model. For
simplicity we only model linear relationships. We will not use this
model to compete against ML but to separate relevant from irrelevant
predictors.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{require}\NormalTok{(MASS)}
\NormalTok{m.mnr.best <-}\StringTok{ }\NormalTok{MASS}\OperatorTok{::}\KeywordTok{stepAIC}\NormalTok{(mnr)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lrr@{}}
\toprule
& AIC & Terms\tabularnewline
\midrule
\endhead
full model & 2954.964 & 53\tabularnewline
best model & 2854.969 & 26\tabularnewline
\bottomrule
\end{longtable}

A chi-square test between the full model and the reduced model is far
from significant. This means that the full model is not better than the
reduced model. This is expected from the the AIC stepwise model search.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{anova}\NormalTok{(m.mnr.best, m.mnr, }\DataTypeTok{test =} \StringTok{'Chisq'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{support-vector-machine}{%
\subsection{Support Vector Machine}\label{support-vector-machine}}

\hypertarget{linear}{%
\subsubsection{linear}\label{linear}}

Each household has a vector of 53 feature values. Let's interpret the
input data as a 53 dimensional space in which the 2006 households are
located. Support Vector Machine constructs hyperplanes that separate the
households by household size. It is a binary classifier, with 5 more
than two classes it runs through all each time separating one against
all others. Usually there is no perfect separation, some households
still lie on the other side of the hyperplane. The best separation out
of many possible solutions is chosen to be the one with the biggest
distance to the closest points. Points within this margin and beyond the
hyperplane are penalized as further apart they are located (hinge loss).
Tuning the error distance with high costs, tends to improve the
classification on training data with the risk of overfitting and bad
classification on new data, et vice versa.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(e1071)}
\NormalTok{m.svm.linear <-}\StringTok{ }\KeywordTok{svm}\NormalTok{(hhsize }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ d}\OperatorTok{$}\NormalTok{train, }\DataTypeTok{kernel =} \StringTok{"linear"}\NormalTok{, }\DataTypeTok{cost =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{radial-kernel}{%
\subsubsection{Radial Kernel}\label{radial-kernel}}

Adding non-linearly transformed features to the input data, increases
the chances that a well separating hyperplane can be found. In theory,
adding enough well transformed features makes any training data
perfectly separable. However such a sophisticated fit to the training
data carries a high risk of poorly performing on new data. We use a
Gaussian kernel to add transformed features on the fly. In contrast to
the non-transformed input data, this would allow to separate households
if they were to form clusters by household size at some dimensions in
the feature space.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m.svm.radial <-}\StringTok{ }\KeywordTok{svm}\NormalTok{(hhsize }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ d}\OperatorTok{$}\NormalTok{train, }\DataTypeTok{kernel =} \StringTok{"radial"}\NormalTok{, }\DataTypeTok{cost =} \DecValTok{15}\NormalTok{, }
                    \DataTypeTok{gamma =} \FloatTok{0.01}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Optimal values for parameters like cost and gamma were be found by
tuning procedures. However what ever is optimized on training data is
not necessarily beneficial for prediction accuracy on new data.

\hypertarget{random-forest}{%
\subsection{Random Forest}\label{random-forest}}

A decision tree is an iterative partitioning of the input data. For each
feature the best binary split is found so that in the subsets the
occurrence of the\\
of household size classes is as homogeneous as possible. The feature
with the best split serves as the first node to split the input data.
For each subset the procedure is repeated resulting in a tree like
partitioning. For prediction, new data is run through the very same
decision rules. The end leafs of the tree represent the final decisions
to which class a new case belongs to. The end leaf might not be pure,
not allowing a deterministic decision for one or the other household
size class, so the majority class from training is taken. Such a single
tree models very specific patterns and has a high risk of overfitting.

Random forest averages the results of many trees. For each tree only a
random subset of observations is used. This bootstrapping increases bias
and reduces variance in each tree, making the final average more
accurate for predicting new data. The aggregation across trees is only
beneficial for accuracy gain if the trees are independent among each
other. At each node the splitting variable is chosen from a random
subsample of all features, introducing some degree of independence among
the trees. Figure \ref{fig:fig10} shows the out of box (OOB) error rate
across the 500 trees that were calculated.

Because the five household size classes or not uniformly distributed we
instruct the Random Forest algorithm to use a stratified subsample for
each tree.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(randomForest)}
\NormalTok{m.rf <-}\StringTok{ }\KeywordTok{randomForest}\NormalTok{(hhsize }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ d}\OperatorTok{$}\NormalTok{train, }\DataTypeTok{imortance =} \OtherTok{TRUE}\NormalTok{, }
                     \DataTypeTok{strata =}\NormalTok{ d}\OperatorTok{$}\NormalTok{train}\OperatorTok{$}\NormalTok{hhsize, }
                     \DataTypeTok{sampsize =} \KeywordTok{rep}\NormalTok{(}\KeywordTok{min}\NormalTok{(}\KeywordTok{table}\NormalTok{(d}\OperatorTok{$}\NormalTok{train}\OperatorTok{$}\NormalTok{hhsize)), }\DecValTok{5}\NormalTok{) }\OperatorTok{*}\StringTok{ }\FloatTok{.7}
\NormalTok{                     )}
\end{Highlighting}
\end{Shaded}

\hypertarget{results}{%
\section{Results}\label{results}}

\hypertarget{performance}{%
\subsection{Performance}\label{performance}}

\hypertarget{accuracy}{%
\subsubsection{Accuracy}\label{accuracy}}

Accuracy was calculated as the average correctly classified households.
Because this formula ignores the different frequencies of household size
classes, the accuracy on small households are more influential in this
overall score. A score of 1 (or 100\%) means perfect match, 0 or (0\%)
not a single match. A random matching process would generate an accuracy
of .27 (or 27\%), as outlined above.

The accuracy on training set is the model fit. More important is the
cross validation with test data. If a trained model performs well on new
data this means the rules for classification are of general importance
and not restricted to train data. The cross validation on the a test
data set also allows to directly compare different models.

\begin{longtable}[]{@{}lrr@{}}
\caption{\label{tab:tab7} The Accuracy in train and test dataset for the
different classifiers.}\tabularnewline
\toprule
& train & test\tabularnewline
\midrule
\endfirsthead
\toprule
& train & test\tabularnewline
\midrule
\endhead
multinomial & 0.56 & 0.42\tabularnewline
randomforest & 0.44 & 0.42\tabularnewline
svm.linear & 0.57 & 0.43\tabularnewline
svm.radial & 0.95 & 0.44\tabularnewline
\bottomrule
\end{longtable}

In all cases accuracy on train data is better than on test data, as
expected. On test data all four models perform very similar. Between
42\% and 44\% of all households in the test data set can be classified
correctly. Given an accuracy of 27\% by randomly guessing this
performance is not good. Apparently it is hard to predict the household
size by TV viewing behavior. At least with the input data used here.

An eye-catching result is the accuracy of the support vector machine
using a radial kernel. With 95\% correct classification this is close to
perfection. The creation of non-linear feature space apparently enables
SVM to find well separating hyperplanes. However this fine tuned pattern
recognition becomes almost useless when applied to new data. The
accuracy on test data is not much better compared to the linear SVM.

\hypertarget{cohens-kappa}{%
\subsubsection{Cohen's Kappa}\label{cohens-kappa}}

Cohen's kappa is an alternative to classical accuracy. Taking into
account the probability of chance within each household size class, it
is seem as somewhat more robust. A kappa of zero means zero match and a
kappa of 1 means perfect match. The interpretation of Cohen's kappa is
not easy because the magnitude changes also by other factors than
agreement.

Cohen's weighted kappa punishes disagreement more as further apart the
categories lie from each other. For example, incorrectly classifying a
household of size 1 as 5 is worse than 2. Weighted kappa makes is useful
if the target variable is seen as an ordinal scale.

\begin{longtable}[]{@{}lrr@{}}
\caption{\label{tab:tab8} Cohen's weighted Kappa in train and test
dataset for the different classifiers.}\tabularnewline
\toprule
& train & test\tabularnewline
\midrule
\endfirsthead
\toprule
& train & test\tabularnewline
\midrule
\endhead
multinomial & 0.64 & 0.48\tabularnewline
randomforest & 0.53 & 0.52\tabularnewline
svm.linear & 0.66 & 0.47\tabularnewline
svm.radial & 0.97 & 0.53\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{confusion-matrix}{%
\subsubsection{Confusion Matrix}\label{confusion-matrix}}

The confusion matrix is the contingency table that results when counting
matching classification between prediction and true condition. While
accuracy and Cohen's kappa return a single overall score, the
visualization of Cohen's agreement matrix gives an insight of the
classification performance for each household size class.

Figure \ref{fig:fig8} shows a heatmap of the agreement matrix on test
data for each classifier. All four classifier yield the same pattern.
The prediction of small households (household size 1 and 2) is moderate
while prediction of the household size larger than two persons is very
poor.

\hypertarget{variance-importance}{%
\subsection{Variance Importance}\label{variance-importance}}

To asses which features are most important for the prediction of
household size, the variance importance was calculated. For Random
Forest the variance importance for a particular variable is the average
score improvement across all nodes and all trees where that variable was
chosen for splitting (MeanDecreaseGini). For the multinomial logistic
model the variance importance is sassed by the sum over the four
absolute coefficients (there are 4 coefficients for 5 classes, household
size 1 being the reference class). Although unbalanced, all five classes
get the same weight for the overall importance in both the MLR and RF
importance measure.

Figure \ref{fig:fig9} in the Appendix shows the 15 most important
variables for the two multinomial logistic models and Random Forest. The
reduced multinomial model shows almost identical ranking as the full
model. This no surprise, as the stepwise model selection keeps important
terms and drops irrelevant terms.

Overall, the set of important variables between Random Forest and
Multinomial Model is quite similar, although the magnitude of importance
differs substantially. Features of all the domains time, channels and
programs are represented in the lists. Most significant difference are
commercial programs and foreign channels that seem important for MNL but
not or Random Forest. For Random Forest viewing children channels and
programs is the most important feature for discriminating household
size.

One reason for differences may be that Multinomial regression did not
model any interaction terms between features. The iterative binary
partitioning by Random Forest on the other hand is able to model all
possible interactions if opportune. But such interactions may differ
form tree to tree and is not described mathematically. However we can
visualize the overall relationship between two variables by partial
dependence plot.

\hypertarget{partial-dependence}{%
\subsection{Partial Dependence}\label{partial-dependence}}

How exactly does one particular feature influence the probability of
falling into one of household size class? Or how does the interaction
between target and feature look like? This partial dependence can be
visualized by fitting target variable for a range of values of the
feature of interest while keeping the values of all other features in
the model constant. Because we are modelling categorical classes, this
has to be repeated for each class separately. When comparing the
dependence of feature and target between different classifiers, we
expect similar dependency pattern. For example, if viewing kids channels
is negatively associated with the probability of household being
classified as 1-person household, this negative relationship is expected
to be apparent in all modelling processes.

Figure \ref{fig:fig11} to \ref{fig:fig14} show dependence plots for two
features: viewing on kids channels and viewing on workdays between 17
and 20 o'clock. As more TV is watched on kids channels as more likely it
becomes that the household is classified as a bigger household (size 3,
4 or 5) et vice versa. As more TV is watched during 17 and 20 o'clock on
workdays, the probability increases of being classified as a small
household (size 1 and 2).

In each case first the effects plot from multinomial linear regression
is shown followed by the partial dependency plot from Random Forest. As
expected the basic relation between target and feature is similar
between models. Random Forest gives a much more precise interaction
curve than multinomial linear regression which is defined to model
harmonic log odd ratios transitions.

\hypertarget{discussion}{%
\section{Discussion}\label{discussion}}

In this study we have tried to predict the household size, e.g.~the
number of individuals living in a home by the TV viewing data that was
recorded by the Mediapulse measurement. For the TV viewing raw data 53
features have been extracted and used as input data for learning
algorithms. The features comprise viewing characteristics like the
daytime, weekday, type of channel and program genre. This input data was
first inspected, transformed and visualized. Three different models for
statistical learning have been compared in terms of prediction
performance: multinomial logistic regression, support vector machine and
random forest. Their performance were assessed by accuracy and Cohen's
kappa. The most important features were identified by variance
importance measures and the dependency of a few features on household
size were inspected by partial dependency plots.

It seems difficult to predict the number of individuals by the TV
viewing behavior. An accuracy of about 43\% with 27\% baseline is rather
poor. The prediction of small households (size 1 and 2) is significantly
better than that of bigger households. To differentiate between a 3, 4
or 5-person home seems almost impossible.

The different classifier do all yield very similar results. Although
they belong to different families of statistical learning algorithms
(tree-based, kernel-based and linear regression model) none of them were
able to outperform the others. It seems rather unlikely that any other
algorithm would yield a significantly better performance.

To some degree the classifiers preferred different features in their
variable importance ranking. Random Forest is much more flexible in
pattern recognition as the multinomial linear regression, particularly
as the latter was specified without interaction terms and nonlinear
transformation terms (despite the log transformation of all features).
It would be interesting to compare the variance importance of SVM, but
no procedure to extract such scores form the SVM model was known to the
author at time of writing.

The input data, 53 different characteristics of TV viewing duration in
seconds was defined based on industry knowledge and intuition.
Inspecting the structure of the input data matrix yielded good
inter-feature variability and no dominant clustering. It would be
possible to add more features such as time-shifted viewing, all 300
channels instead of channel groups or a more sophisticated partitioning
of daytime and weekdays. Basically the performance of machine learning
algorithms can only increase with more features. However we old not
expect a significant improvement, believing that most of relevant
characteristics of TV viewing is captured by the 53 features.

It would be interesting to replace the target variable household size by
an alternative proxy of household composition. For example a
classification of the type of household such as older couple, family
with young / older kids, shared apartments, single mother / father
household, etc. This type of household composition also includes age and
gender information, which is not the case with household size. Maybe it
is easier to distinguish these types. Although for the ultimate goal of
assigning RPD TV viewing to individuals with age and sex attributes, it
is not clear how such alternative household composition would be
helpful.

\pagebreak

\hypertarget{appendix}{%
\section{Appendix}\label{appendix}}

\begin{figure}[H]

{\centering \includegraphics[width=0.75\linewidth]{../data/tv-week} 

}

\caption{\label{fig:fig1} The sum of TV viewing duration [seconds] by weekdays during 2017. On weekends more TV is watched than during the rest of the week. Festival days often behave like Sundays.}\label{fig:unnamed-chunk-21}
\end{figure}

\begin{figure}[H]

{\centering \includegraphics[width=0.75\linewidth]{../data/tv-day} 

}

\caption{\label{fig:fig2} The relative amount of TV viewing across time of the day. The curve is the average of all 365 days in 2017. In the market the peak around 20:00 o'clock is called Primetime. On weekends the curve is flatter.}\label{fig:unnamed-chunk-22}
\end{figure}

\pagebreak

\begin{figure}
\centering
\includegraphics{Diploma_files/figure-latex/unnamed-chunk-23-1.pdf}
\caption{\label{fig:fig3} Illustration of log transformation. The upper
row of scatterplots shows three examples of feature pairs. On of for
each domain \emph{time}, \emph{channel} and \emph{program}. In many
cases viewing duration is not symmetrical distributed. The lower row
shows the very same scatterplot with log transformed values.}
\end{figure}

\pagebreak

\begin{figure}
\centering
\includegraphics{Diploma_files/figure-latex/unnamed-chunk-24-1.pdf}
\caption{\label{fig:fig4} Shown are the same three scatterplots as in
the Figure above but this time the corresponding household size is
indicated by the color of the dots. If there was a feature that would
separate the households (dots) into clusters of the same color, this
would tell us that this particular variable is a good discriminator for
household size. Apprently the variable \emph{chn\_kids} separates black
and red dots from light and dark blue dots. This means there is a
tendency that the more a household watches TV on typical kids channels,
the more likely it is a 4 or 5 person household.}
\end{figure}

\pagebreak

\begin{figure}
\centering
\includegraphics{Diploma_files/figure-latex/unnamed-chunk-25-1.pdf}
\caption{\label{fig:fig5} Heatmap of the correlation matrix of log
transformed input data. Yellow is positive, red is negative correlation.
Household size is included as numeric scale 1 to 5.}
\end{figure}

\pagebreak

\begin{figure}
\centering
\includegraphics{Diploma_files/figure-latex/unnamed-chunk-26-1.pdf}
\caption{\label{fig:fig6} Principal Component Analysis (PCA). The
screeplots shows the variance explained by the first 10 principal
components (PCs). The first PC explaines 30\% of the total variance.}
\end{figure}

\pagebreak

\begin{figure}
\centering
\includegraphics{Diploma_files/figure-latex/unnamed-chunk-27-1.pdf}
\caption{\label{fig:fig7} PCA scatterplots and biplots of the first 3
PCs.}
\end{figure}

\pagebreak

\begin{figure}
\centering
\includegraphics{Diploma_files/figure-latex/unnamed-chunk-28-1.pdf}
\caption{\label{fig:fig8} Heatmaps of Cohen's agreement matrix for each
model.}
\end{figure}

\pagebreak

\begin{figure}
\centering
\includegraphics{Diploma_files/figure-latex/unnamed-chunk-29-1.pdf}
\caption{\label{fig:fig9} Variance Importance Plots for the multinomial
full model, the reduced model and random forest.}
\end{figure}

\pagebreak

\begin{figure}
\centering
\includegraphics{Diploma_files/figure-latex/unnamed-chunk-30-1.pdf}
\caption{\label{fig:fig10} The Figure shows the Random Forest learning
curve for predicting each household size class. The performance at each
tree is calculated Out-Of-Bag (OOB), i.e.~on the remaining cases that
were not selected for training (cross validation).}
\end{figure}

\pagebreak

\begin{figure}

{\centering \includegraphics[width=0.55\linewidth]{../data/partialplot.rf.chn_kids} 

}

\caption{\label{fig:fig11} Random forest partial dependence plot for the variable kids-channel.}\label{fig:unnamed-chunk-31}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.55\linewidth]{../data/partialplot.mnr.chn_kids} 

}

\caption{\label{fig:fig12} Multinomial Linear Model effects plot for the same variable as obove.}\label{fig:unnamed-chunk-32}
\end{figure}

\pagebreak

\textbackslash{}begin\{figure\}

\{\centering \includegraphics[width=0.55\linewidth]{../data/partialplot.rf.day_workday_17to20}

\}

\textbackslash{}caption\{\label{fig:fig13} Random forest partial
dependence plot for the amount of TV viewing between 17 and 20 o'clock
during workdays (variable
day\_workday\_17to20).\}\label{fig:unnamed-chunk-33}
\textbackslash{}end\{figure\}

\begin{figure}

{\centering \includegraphics[width=0.55\linewidth]{../data/partialplot.mnr.day_workday_17to20} 

}

\caption{\label{fig:fig14} Multinomial Linear Model effects plot for the same variable as obove.}\label{fig:unnamed-chunk-34}
\end{figure}


\end{document}
